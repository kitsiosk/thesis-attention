{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "import dlib\n",
    "import yaml\n",
    "from imutils import face_utils\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open('data_cleaned.json') as json_file:\n",
    "    data_all = json.load(json_file)\n",
    "# Extract the keys in sorted order\n",
    "keys_all = sorted(data_all)\n",
    "# Convert python list to np array\n",
    "keys_all = np.asarray(keys_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = glob.glob('dataset/*')\n",
    "# Define global variables\n",
    "NUM_OF_PARTICIPANTS = len(participants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('uuid_lengths_all.json', 'r') as fp:\n",
    "    uuid_lengths = json.load(fp)\n",
    "\n",
    "uuids = []\n",
    "sum = 0\n",
    "for i in range(NUM_OF_PARTICIPANTS):\n",
    "    uuid = participants[i].split('/')[1]\n",
    "    uuids.append(uuid)\n",
    "    sum += uuid_lengths[uuid]\n",
    "DATASET_SIZE = sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2728\n"
     ]
    }
   ],
   "source": [
    "print(DATASET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_camera_parameters(uuid):\n",
    "    with open(\"dataset/\" + uuid + \"/data.yml\", 'r') as stream:\n",
    "        yml_data = yaml.safe_load(stream)\n",
    "        height = yml_data['height']\n",
    "        width  = yml_data['width']\n",
    "    size = [height, width]\n",
    "    focal_length = size[1]\n",
    "    center = (size[1]/2, size[0]/2)\n",
    "    camera_matrix = np.array(\n",
    "                             [[focal_length, 0, center[0]],\n",
    "                             [0, focal_length, center[1]],\n",
    "                             [0, 0, 1]], dtype = \"double\"\n",
    "                             )\n",
    "    dist_coeffs = np.zeros((4,1)) # Assuming no lens distortion\n",
    "\n",
    "    return camera_matrix, dist_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  0]\n",
      " [ 8 10]]\n",
      "#0 TEST SVM: 0.76, TRAIN SVM: 0.69\n",
      "#0 TEST RF: 0.45, TRAIN RF: 1.0\n",
      "[[ 0  9]\n",
      " [ 0 22]]\n",
      "#1 TEST SVM: 0.71, TRAIN SVM: 0.69\n",
      "#1 TEST RF: 0.19, TRAIN RF: 1.0\n",
      "[[ 0 10]\n",
      " [ 0 12]]\n",
      "#2 TEST SVM: 0.55, TRAIN SVM: 0.69\n",
      "#2 TEST RF: 0.32, TRAIN RF: 1.0\n",
      "[[0 4]\n",
      " [0 9]]\n",
      "#3 TEST SVM: 0.69, TRAIN SVM: 0.69\n",
      "#3 TEST RF: 0.69, TRAIN RF: 1.0\n",
      "[[ 0 15]\n",
      " [ 0 22]]\n",
      "#4 TEST SVM: 0.59, TRAIN SVM: 0.69\n",
      "#4 TEST RF: 0.41, TRAIN RF: 1.0\n",
      "[[40  9]\n",
      " [ 4 46]]\n",
      "#5 TEST SVM: 0.87, TRAIN SVM: 0.69\n",
      "#5 TEST RF: 0.57, TRAIN RF: 1.0\n",
      "[[13  5]\n",
      " [ 9 15]]\n",
      "#6 TEST SVM: 0.67, TRAIN SVM: 0.7\n",
      "#6 TEST RF: 0.55, TRAIN RF: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy metrics for the whole dataset. These are computed\n",
    "# by leaving every Subject out one time, calculating the accuracy for each\n",
    "# one and then taking the mean value.\n",
    "accuracy_rf_total = 0\n",
    "accuracy_svm_total = 0\n",
    "\n",
    "num_features = 142\n",
    "\n",
    "dataset_len = 0\n",
    "# Loop over each participant\n",
    "for j in range(10):\n",
    "    \n",
    "    # Extract the UUID\n",
    "    uuid_excluded = uuids[j]\n",
    "    length_excluded = uuid_lengths[uuid_excluded]\n",
    "    \n",
    "    features_train = np.zeros((DATASET_SIZE  - length_excluded, num_features))\n",
    "    features_val = np.zeros((length_excluded, num_features))\n",
    "    labels_train = np.zeros(DATASET_SIZE - length_excluded)\n",
    "    labels_val = np.zeros(length_excluded)\n",
    "    \n",
    "    start = 0\n",
    "    for i in range(NUM_OF_PARTICIPANTS):\n",
    "        uuid = uuids[i]\n",
    "        length = uuid_lengths[uuid]\n",
    "        \n",
    "        image_points_subj = np.load('FAN_landmarks/image_points_' + uuid + '.npy')\n",
    "        model_points_subj = np.load('FAN_landmarks/model_points_' + uuid + '.npy')\n",
    "        labels            = np.load('FAN_landmarks/labels_' + uuid + '.npy')\n",
    "        \n",
    "        if image_points_subj.shape[0] != length:\n",
    "            print('Error')\n",
    "            print(image_points_subj.shape[0], length, j, i, labels.shape[0])\n",
    "            print(uuid)\n",
    "            break\n",
    "            \n",
    "        features = np.zeros((length, num_features))\n",
    "\n",
    "        \n",
    "        camera_matrix, dist_coeffs = get_camera_parameters(uuid)\n",
    "        for k in range(length):\n",
    "            image_points = image_points_subj[k, :]\n",
    "            model_points = model_points_subj[k, :]\n",
    "            camera_matrix, dist_coeffs = get_camera_parameters(uuid)\n",
    "            (success, rotation_vector, translation_vector) = cv2.solvePnP(\n",
    "                model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE\n",
    "            )\n",
    "            \n",
    "            features[k, :] = np.reshape(np.concatenate((rotation_vector.squeeze(), translation_vector.squeeze(),\n",
    "                                                        image_points[:, 0], image_points[:, 1]), axis=0), (142, ))\n",
    "                    \n",
    "        if uuid == uuid_excluded:\n",
    "            features_val = features\n",
    "            labels_val   = labels\n",
    "\n",
    "        else:\n",
    "            features_train[start : start + length] = features\n",
    "            labels_train[start : start + length] = labels\n",
    "            start += length\n",
    "            \n",
    "    svm_classifier = svm.SVC(C=100, kernel='rbf', gamma='scale', probability=True)\n",
    "    svm_classifier.fit(features_train, labels_train)\n",
    "    # Predict SVM with threshold at 0.3 instead of 0.5\n",
    "    threshold = 0.3\n",
    "    y_prob_svm = svm_classifier.predict_proba(features_val)\n",
    "    y_pred_svm = (y_prob_svm[:, 1] >= threshold).astype(int)\n",
    "    \n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "    rf_classifier.fit(features_train, labels_train)\n",
    "    y_pred_rf = rf_classifier.predict(features_val)\n",
    "        \n",
    "    accuracy_svm_subject = metrics.accuracy_score(labels_val, y_pred_svm)\n",
    "    accuracy_rf_subject = metrics.accuracy_score(labels_val, y_pred_rf)\n",
    "    accuracy_svm_total += accuracy_svm_subject*y_pred_svm.shape[0]\n",
    "    accuracy_rf_total += accuracy_svm_subject*y_pred_svm.shape[0]\n",
    "    dataset_len += y_pred.shape[0]\n",
    "    \n",
    "    print('#{} TEST SVM: {}, TRAIN SVM: {}'.format(j, round(accuracy_svm_subject, 2),\n",
    "                                      round(svm_classifier.score(features_train, labels_train), 2)))\n",
    "    print('#{} TEST RF: {}, TRAIN RF: {}'.format(j, round(accuracy_rf_subject, 2),\n",
    "                                      round(rf_classifier.score(features_train, labels_train), 2)))\n",
    "#     plot_learning_curve(classifier,'Learning Curve',  features_train, labels_train, cv=5)\n",
    "#     plt.show()\n",
    "print(accuracy_svm_total/dataset_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4228855721393035\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_svm_total/dataset_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
