{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Prediction in a non-calibrated system\n",
    "\n",
    "The purpose of this notebook is to predict a user's attention, meaning a model to decide whether a user is looking on the laptop's screen or not.  We want to achieve this without calibrating the system and by only using 2D frames from the laptop camera in real-time. \n",
    "\n",
    "For the thesis, we have created a dataset with labelled frames(1: looking, 0: not looking) and performed a couple of preprocessing steps to extract 68 facial landmarks on the face and the 2 iris points calculated by Loceye's eye-tracking algorithms. \n",
    "Below, we will try out different data architectures and classifiers to figure out the best performance.\n",
    "To achieve real-time performance and non-biased behaviour, we do not want to include any calibrating process. Therefore we will approximate a mapping between the 2D camera points and the 3D world by solving the [Perspective-n-Point](https://en.wikipedia.org/wiki/Perspective-n-Point) problem using the built-in OpenCV method solvePnP. This way, we will obtain the Rotation and Translation matrix(3x1 each) for each frame. Afterwards, we will use them in different architectures to train a machine learning model that will predict the final output. The architectures presented below are:\n",
    "\n",
    " - Feed the raw two vectors in a classifier.\n",
    " - Feed the two vectors and the 2 iris points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "import glob\n",
    "import pickle\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training examples to use(0-2806)\n",
    "DATASET_SIZE = 2758\n",
    "DEBUG = False\n",
    "\n",
    "# Load the dataset\n",
    "with open('data_cleaned.json') as json_file:\n",
    "    data_all = json.load(json_file)\n",
    "\n",
    "# Extract the keys in sorted order\n",
    "keys = sorted(data_all)\n",
    "\n",
    "# Convert python list to np array\n",
    "keys = np.asarray(keys)\n",
    "print(keys.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that returns the required parameters of the solvePnP method. The paramaters *model_points* and *dist_coeffs* are the same for every example so they are declared globally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D model points.\n",
    "model_points = np.array([\n",
    "                            (0.0, 0.0, 0.0),             # Nose tip\n",
    "                            (0.0, -330.0, -65.0),        # Chin\n",
    "                            (-225.0, 170.0, -135.0),     # Left eye left corner\n",
    "                            (225.0, 170.0, -135.0),      # Right eye right corne\n",
    "                            (-150.0, -150.0, -125.0),    # Left Mouth corner\n",
    "                            (150.0, -150.0, -125.0)      # Right mouth corner\n",
    "                        ])\n",
    "\n",
    "def generate_solvepnp_parameters(size, landmarks):\n",
    "    # Approximate camera intrinsic parameters\n",
    "    focal_length = size[1]\n",
    "    center = (size[1]/2, size[0]/2)\n",
    "    camera_matrix = np.array(\n",
    "                             [[focal_length, 0, center[0]],\n",
    "                             [0, focal_length, center[1]],\n",
    "                             [0, 0, 1]], dtype = \"double\"\n",
    "                             )\n",
    "\n",
    "    # Grab the 2D coordinates of our six sample points\n",
    "    image_points = np.array([\n",
    "        (landmarks[33]['x'], landmarks[33]['y']) ,     # Nose tip\n",
    "        (landmarks[8]['x'], landmarks[8]['y']),     # Chin\n",
    "        (landmarks[36]['x'], landmarks[36]['y']),     # Left eye left corner\n",
    "        (landmarks[45]['x'], landmarks[45]['y']),     # Right eye right corner\n",
    "        (landmarks[48]['x'], landmarks[48]['y']),     # Left Mouth corner\n",
    "        (landmarks[54]['x'], landmarks[54]['y'])      # Right mouth corner\n",
    "    ], dtype=\"double\")\n",
    "    # 3D model points.\n",
    "    \n",
    "    return image_points, camera_matrix\n",
    "\n",
    "model_points = np.array([\n",
    "                            (0.0, 0.0, 0.0),             # Nose tip\n",
    "                            (0.0, -330.0, -65.0),        # Chin\n",
    "                            (-225.0, 170.0, -135.0),     # Left eye left corner\n",
    "                            (225.0, 170.0, -135.0),      # Right eye right corne\n",
    "                            (-150.0, -150.0, -125.0),    # Left Mouth corner\n",
    "                            (150.0, -150.0, -125.0)      # Right mouth corner\n",
    "                        ])\n",
    "\n",
    "dist_coeffs = np.zeros((4,1)) # Assuming no lens distortion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging function used to visualize image points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image(im, rotation_vector, translation_vector, camera_matrix, image_points):\n",
    "    # Project a 3D point (0, 0, 1000.0) onto the image plane.\n",
    "    # We use this to draw a line sticking out of the nose\n",
    "    (nose_end_point2D, jacobian) = cv2.projectPoints(\n",
    "        np.array([(0.0, 0.0, 500.0)]), rotation_vector, translation_vector, camera_matrix, dist_coeffs\n",
    "        )\n",
    "    for p in image_points:\n",
    "        cv2.circle(im, (int(p[0]), int(p[1])), 3, (0,0,255), -1)\n",
    "    p1 = ( int(image_points[0][0]), int(image_points[0][1]) )\n",
    "    p2 = ( int(nose_end_point2D[0][0][0]), int(nose_end_point2D[0][0][1]) )\n",
    "\n",
    "    # Draw a line connecting the two points. This line must show\n",
    "    # the direction out of the nose\n",
    "    cv2.line(im, p1, p2, (255,0,0), 2)\n",
    "    # Display image\n",
    "    cv2.imshow(\"Output\", im)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Rotation Vector:\\n {0}\".format(rotation_vector))\n",
    "    print(\"Translation Vector:\\n {0}\".format(translation_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture #1\n",
    "\n",
    "#### In this data architecture we will use as input the translation vector and rotation vector, each one of shape (3, 1) so a signle training example of our dataset will be of shape (6, 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((DATASET_SIZE, 6, 1))\n",
    "y = np.zeros(DATASET_SIZE)\n",
    "\n",
    "# Keep track of the indices where solvePnP crashed\n",
    "failed_indices = []\n",
    "\n",
    "for i in range(DATASET_SIZE):\n",
    "    key = keys[i]\n",
    "\n",
    "    # Approximate camera intrinsic parameters\n",
    "    im = cv2.imread('dataset/' + key)   # This imread is time consuming! Another way?\n",
    "    size = im.shape\n",
    "    landmarks = data_all[key]['landmarks']\n",
    "    \n",
    "    image_points, camera_matrix = generate_solvepnp_parameters(size, landmarks)\n",
    "    \n",
    "    # Solve the PnP problem with the parameters specified above\n",
    "    # and obtain rotation and translation vectors\n",
    "    (success, rotation_vector, translation_vector) = cv2.solvePnP(\n",
    "        model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_DLS\n",
    "        )\n",
    "    \n",
    "    X[i, :] = np.concatenate((rotation_vector, translation_vector), axis=0)\n",
    "    \n",
    "    # Check if solvePnP crashed\n",
    "    if(X[i, 0] > 10000):\n",
    "        print(key)\n",
    "        failed_indices.append(i)\n",
    "    \n",
    "    # Check if it is positive or negative example\n",
    "    output = key.split('/')[1]\n",
    "    if(output == 'positive'):\n",
    "        y[i] = 1\n",
    "    elif(output == 'negative'):\n",
    "        y[i] = 0\n",
    "        \n",
    "# # Remove indices where solvePnP crashed\n",
    "X = np.delete(X, failed_indices, axis=0)\n",
    "y = np.delete(y, failed_indices, axis=0)\n",
    "DATASET_SIZE = X.shape[0]\n",
    "\n",
    "X = X.squeeze()\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preproccessing: Normalize features to have 0 mean and 1 Std\n",
    "We obtain the mean and std from the first 800 examples and not the whole dataset beacause computations crush with too many exmaples and give infinite mean and std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Before normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X.mean(axis=0)\n",
    "std = X.std(axis=0)\n",
    "\n",
    "# # Save the mean and std for predictions in other notebooks\n",
    "# with open('mean.pickle', 'wb') as f:\n",
    "#     pickle.dump(m, f, pickle.HIGHEST_PROTOCOL)\n",
    "# with open('std.pickle', 'wb') as f:\n",
    "#     pickle.dump(std, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize features to have zero mean and unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = (X-m)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train different classifiers\n",
    "Here we will try out 3 classifiers and compare their results:\n",
    " - A SVM with rbf kernel and penalty parameter C=100\n",
    " - A Logistic Regression classifer\n",
    " - A Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classifiers to train in order to summarize the results\n",
    "NUM_OF_CLASSIFIERS = 10\n",
    "Y_SVM, Y_LR, Y_RF = np.zeros((NUM_OF_CLASSIFIERS)), np.zeros((NUM_OF_CLASSIFIERS)), np.zeros((NUM_OF_CLASSIFIERS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "for i in range(NUM_OF_CLASSIFIERS):\n",
    "    # Try a different train/test split each time\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)\n",
    "    \n",
    "    svm_classifier = svm.SVC(C=100, kernel='rbf', gamma='auto')\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    y_pred_svm = svm_classifier.predict(X_test)\n",
    "    Y_SVM[i] = metrics.accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "# Print the last result\n",
    "print('Training set accuracy for SVM:', svm_classifier.score(X_train, y_train))\n",
    "print('Test set accuracy for SVM: ', metrics.accuracy_score(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "for i in range(NUM_OF_CLASSIFIERS):\n",
    "    # Try a different train/test split each time\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)\n",
    "    \n",
    "    lr_classifier = LogisticRegression(solver='lbfgs')\n",
    "    lr_classifier.fit(X_train, y_train)\n",
    "    y_pred_lr = lr_classifier.predict(X_test)\n",
    "    Y_LR[i] = metrics.accuracy_score(y_test, y_pred_lr)\n",
    "\n",
    "print('Training set accuracy for Logistic Regression:', lr_classifier.score(X_train, y_train))\n",
    "print('Test set accuracy for Logistic Regression: ', metrics.accuracy_score(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forst Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "for i in range(NUM_OF_CLASSIFIERS):\n",
    "    # Try a different train/test split each time\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)\n",
    "    \n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    y_pred_rf = rf_classifier.predict(X_test)\n",
    "    Y_RF[i] = metrics.accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "print('Training set accuracy for Random Forst:', rf_classifier.score(X_train, y_train))\n",
    "print('Test set accuracy for Random Forest: ', metrics.accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize & Summarize results for Architecture #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained the 3 classifiers for 10 different train/test splits and the accuracy results are shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t = np.linspace(0, NUM_OF_CLASSIFIERS, NUM_OF_CLASSIFIERS)\n",
    "plt.plot(t, Y_SVM, 'r')\n",
    "plt.plot(t, Y_LR, 'g')\n",
    "plt.plot(t, Y_RF, 'b')\n",
    "plt.xlabel('classifier No')\n",
    "plt.ylabel('Test set Accuracy')\n",
    "plt.legend(('SVM', 'LR', 'RF'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(Y_SVM), np.max(Y_RF))\n",
    "print(np.mean(Y_SVM), np.mean(Y_RF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best out of 10 scores for each classifier is presented below\n",
    "\n",
    "Metric                | SVM        | Logistic Regression | Random Forest |\n",
    ":----:                | :--------: | :-----------------: | :-----:       |\n",
    "Training set Accuracy | 0.743      | 0.619               | 1             | \n",
    "Test set Accuracty    | 0.699      | 0.612               | 0.779         |\n",
    "\n",
    "We clearly see that the Random Forest classifier with 100 trees outperfomrs the other 2, reaching 78% accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classifier.pickle', 'wb') as f:\n",
    "    pickle.dump(rf_classifier, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture #2\n",
    "#### In this data architecture we will use as input the translation vector and rotation vector plus the 2 iris points) so a signle training example of our dataset will be of shape (10, 1). \n",
    "We already have the array X of shape (6, 1) containing the rotation and translation vectors stacked so we only need to extract the iris points from our data.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = np.zeros((DATASET_SIZE, 10))\n",
    "for i in range(DATASET_SIZE):\n",
    "    key = keys[i]\n",
    "    iris_right = np.asarray(data_all[key]['iris_right'])\n",
    "    iris_left = np.asarray(data_all[key]['iris_left'])\n",
    "    X2[i, :] = np.concatenate((X[i, :], iris_left, iris_right), axis = 0)\n",
    "print(X2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps are the same as for the first architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X2.mean(axis=0)\n",
    "std = X2.std(axis=0)\n",
    "\n",
    "X2_scaled = (X2-m)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train different classifiers\n",
    "Here we will try out 3 classifiers and compare their results:\n",
    " - A SVM with rbf kernel and penalty parameter C=100\n",
    " - A Logistic Regression classifer\n",
    " - A Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classifiers to train in order to summarize the results\n",
    "NUM_OF_CLASSIFIERS = 10\n",
    "Y_SVM2, Y_LR2, Y_RF2 = np.zeros((NUM_OF_CLASSIFIERS)), np.zeros((NUM_OF_CLASSIFIERS)), np.zeros((NUM_OF_CLASSIFIERS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "for i in range(NUM_OF_CLASSIFIERS):\n",
    "    # Try a different train/test split each time\n",
    "    X2_train, X2_test, y2_train, y2_test = train_test_split(X2_scaled, y, test_size=0.2)\n",
    "    \n",
    "    svm_classifier2 = svm.SVC(C=100, kernel='rbf', gamma='auto')\n",
    "    svm_classifier2.fit(X2_train, y2_train)\n",
    "    y2_pred_svm = svm_classifier2.predict(X2_test)\n",
    "    Y_SVM2[i] = metrics.accuracy_score(y2_test, y2_pred_svm)\n",
    "\n",
    "print('Training set accuracy for SVM:', svm_classifier2.score(X2_train, y2_train))\n",
    "print('Test set accuracy for SVM: ', metrics.accuracy_score(y2_test, y2_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "for i in range(NUM_OF_CLASSIFIERS):\n",
    "    # Try a different train/test split each time\n",
    "    X2_train, X2_test, y2_train, y2_test = train_test_split(X2_scaled, y, test_size=0.2)\n",
    "    \n",
    "    lr_classifier2 = LogisticRegression(solver='lbfgs')\n",
    "    lr_classifier2.fit(X2_train, y2_train)\n",
    "    y2_pred_lr = lr_classifier2.predict(X2_test)\n",
    "    Y_LR2[i] = metrics.accuracy_score(y2_test, y2_pred_lr)\n",
    "\n",
    "print('Training set accuracy for Logistic Regression:', lr_classifier2.score(X2_train, y2_train))\n",
    "print('Test set accuracy for Logistic Regression: ', metrics.accuracy_score(y2_test, y2_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "for i in range(NUM_OF_CLASSIFIERS):\n",
    "    # Try a different train/test split each time\n",
    "    X2_train, X2_test, y2_train, y2_test = train_test_split(X2_scaled, y, test_size=0.2)\n",
    "    \n",
    "    rf_classifier2 = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "    rf_classifier2.fit(X2_train, y2_train)\n",
    "    y2_pred_rf = rf_classifier2.predict(X2_test)\n",
    "    Y_RF2[i] = metrics.accuracy_score(y2_test, y2_pred_rf)\n",
    "\n",
    "print('Training set accuracy for Random Forst:', rf_classifier2.score(X2_train, y2_train))\n",
    "print('Test set accuracy for Random Forest: ', metrics.accuracy_score(y2_test, y2_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize & Summarize results for Architecture #2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained the 3 classifiers for 25 different train/test splits and the accuracy results are shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t = np.linspace(0, NUM_OF_CLASSIFIERS, NUM_OF_CLASSIFIERS)\n",
    "plt.plot(t, Y_SVM2, 'r')\n",
    "plt.plot(t, Y_LR2, 'g')\n",
    "plt.plot(t, Y_RF2, 'b')\n",
    "plt.xlabel('classifier No')\n",
    "plt.ylabel('Test set Accuracy')\n",
    "plt.legend(('SVM', 'LR', 'RF'))\n",
    "\n",
    "plt.show()\n",
    "print(Y_SVM2.max(axis=0), Y_LR2.max(axis=0), Y_RF2.max(axis=0))\n",
    "print(Y_SVM2.mean(axis=0), Y_LR2.mean(axis=0), Y_RF2.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric                | SVM        | Logistic Regression | Random Forest |\n",
    ":----:                | :--------: | :-----------------: | :-----:       |\n",
    "Training set Accuracy | 0.752      | 0.533               | 1             | \n",
    "Test set Accuracty    | 0.716      | 0.588               | 0.770         |\n",
    "\n",
    "The difference between the first and the second architecture is noticeable.\n",
    "The SVM  method is more accurate now approaching the Random Forest accuracy, which also improved in but not as much as the SVM. Also, the Logistic Regression classifier seems unable to handle our Dataset in both architectures; therefore, we will not consider it for the next of our research. The best accuracy scored, 77%, is achieved by Random Forest classifier in our second data architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture #3\n",
    "In this archtecture we will use the difference vector between the iris and the inner edge point of the eye(Maybe add and image here?). First, we will keep the raw iris coordinates and add the 2 vectors to our training data, yielding to a shape of (14, 1). Afterwards we will try removing the absolute coordinates of the iris and compare the 2 results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously on this notebook we had extracted the 68 facial landmarks as well as the two iris points. The 2 inner eye points are the landmarks 39 and 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset construction\n",
    "\n",
    "X3 = np.zeros((DATASET_SIZE, 14))\n",
    "for i in range(DATASET_SIZE):\n",
    "    key = keys[i]\n",
    "    \n",
    "    landmarks = data_all[key]['landmarks']\n",
    "    iris_right = np.asarray(data_all[key]['iris_right'])\n",
    "    iris_left = np.asarray(data_all[key]['iris_left'])\n",
    "    \n",
    "    left_vector = np.asarray( (abs(iris_left[0] - landmarks[39]['x']), abs(iris_left[1] - landmarks[39]['y'])) )\n",
    "    right_vector = np.asarray( (abs(iris_right[0] - landmarks[42]['x']), abs(iris_right[1] - landmarks[42]['y'])) )\n",
    "    X3[i] = np.concatenate((X2[i], left_vector, right_vector), axis=0)\n",
    "    \n",
    "    if DEBUG:\n",
    "        im = cv2.imread('dataset/' + key)\n",
    "        cv2.circle(im, (int(iris_left[0]), int(iris_left[1])), 3, (0,0,255), -1)\n",
    "        cv2.circle(im, (int(iris_right[0]), int(iris_right[1])), 3, (0,0,255), -1)\n",
    "        cv2.circle(im, (landmarks[39]['x'], landmarks[39]['y']), 3, (0,0,255), -1)\n",
    "        cv2.circle(im, (landmarks[42]['x'], landmarks[42]['y']), 3, (0,0,255), -1)\n",
    "        cv2.imshow('Im', im)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "print(X3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X3.mean(axis=0)\n",
    "std = X3.std(axis=0)\n",
    "X3_scaled = (X3-m)/std\n",
    "\n",
    "print(m)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classifiers to train in order to summarize the results\n",
    "NUM_OF_CLASSIFIERS = 10\n",
    "Y_SVM3, Y_LR3, Y_RF3 = np.zeros((NUM_OF_CLASSIFIERS)), np.zeros((NUM_OF_CLASSIFIERS)), np.zeros((NUM_OF_CLASSIFIERS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "for i in range(NUM_OF_CLASSIFIERS):\n",
    "    # Try a different train/test split each time\n",
    "    X3_train, X3_test, y3_train, y3_test = train_test_split(X3_scaled, y, test_size=0.2)\n",
    "    \n",
    "    svm_classifier3 = svm.SVC(C=10, kernel='rbf', gamma='auto')\n",
    "    svm_classifier3.fit(X3_train, y3_train)\n",
    "    y3_pred_svm = svm_classifier3.predict(X3_test)\n",
    "    Y_SVM3[i] = metrics.accuracy_score(y3_test, y3_pred_svm)\n",
    "\n",
    "# Print the last result\n",
    "print('Training set accuracy for SVM:', svm_classifier3.score(X3_train, y3_train))\n",
    "print('Test set accuracy for SVM: ', metrics.accuracy_score(y3_test, y3_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "for i in range(NUM_OF_CLASSIFIERS):\n",
    "    # Try a different train/test split each time\n",
    "    X3_train, X3_test, y3_train, y3_test = train_test_split(X3_scaled, y, test_size=0.2)\n",
    "    \n",
    "    rf_classifier3 = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "    rf_classifier3.fit(X3_train, y3_train)\n",
    "    y3_pred_rf = rf_classifier3.predict(X3_test)\n",
    "    Y_RF3[i] = metrics.accuracy_score(y3_test, y3_pred_rf)\n",
    "\n",
    "print('Training set accuracy for Random Forst:', rf_classifier3.score(X3_train, y3_train))\n",
    "print('Test set accuracy for Random Forest: ', metrics.accuracy_score(y3_test, y3_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize & Summarize results for Architecture #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t = np.linspace(0, NUM_OF_CLASSIFIERS, NUM_OF_CLASSIFIERS)\n",
    "plt.plot(t, Y_SVM3, 'r')\n",
    "plt.plot(t, Y_RF3, 'b')\n",
    "plt.xlabel('classifier No')\n",
    "plt.ylabel('Test set Accuracy')\n",
    "plt.legend(('SVM', 'RF'))\n",
    "\n",
    "plt.show()\n",
    "print(Y_SVM3.max(axis=0), Y_RF3.max(axis=0))\n",
    "print(Y_SVM3.mean(axis=0), Y_RF3.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we also tried feeding the difference vectors only without the absolute iris location, reaching an overall accuracy 2% lower than the one above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to save our trained Random Forest classifier as it is the more accurate one and use it in external applications. The cell below will save the last trained classifier and not the best one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pickle', 'wb') as f:\n",
    "    pickle.dump(rf_classifier3, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
