{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave-One-Subject-Out Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "import glob\n",
    "import pickle\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Model points for SolvePnP\n",
    "We will approximate the 3D points of the following face parts with the correspoinding coordinates. This is a general model of the human face and we do not need to worry much about absolute accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_camera_parameters(size):\n",
    "    focal_length = size[1]\n",
    "    center = (size[1]/2, size[0]/2)\n",
    "    camera_matrix = np.array(\n",
    "                             [[focal_length, 0, center[0]],\n",
    "                             [0, focal_length, center[1]],\n",
    "                             [0, 0, 1]], dtype = \"double\"\n",
    "                             )\n",
    "    dist_coeffs = np.zeros((4,1)) # Assuming no lens distortion\n",
    "    \n",
    "    return camera_matrix, dist_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_image_points(landmarks):\n",
    "    image_points = np.zeros((68, 2))\n",
    "\n",
    "    for i in range(68):\n",
    "        image_points[i, :] = (landmarks[i]['x'], landmarks[i]['y'])\n",
    "    \n",
    "    return image_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_model_points(filename='model_points.txt'):\n",
    "    \"\"\"Get all 68 3D model points from file\"\"\"\n",
    "    raw_value = []\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            raw_value.append(line)\n",
    "    model_points = np.array(raw_value, dtype=np.float32)\n",
    "    model_points = np.reshape(model_points, (3, -1)).T\n",
    "\n",
    "    # Transform the model into a front view.\n",
    "    model_points[:, 2] *= -1\n",
    "\n",
    "    return model_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image(im, rotation_vector, translation_vector, image_points, camera_matrix):\n",
    "    for point in image_points:\n",
    "        cv2.circle(im, (int(point[0]), int(point[1])), 3, (0, 0, 255), -1)\n",
    "    cv2.circle(im, (int(iris_left[0]), int(iris_left[1])), 3, (0, 0, 255), -1)\n",
    "    cv2.circle(im, (int(iris_right[0]), int(iris_right[1])), 3, (0, 0, 255), -1)\n",
    "    \n",
    "    # Project the 3D point (0.55592, 6.5629, 300.0) onto the image plane.\n",
    "    # We use this to draw a line sticking out of the nose\n",
    "    (nose_end_point2D, jacobian) = cv2.projectPoints(\n",
    "        np.array([(0.55592, 6.5629, 300.0)]), rotation_vector, translation_vector, camera_matrix, dist_coeffs\n",
    "        )\n",
    "    # Draw a line connecting the two points. This line must show\n",
    "    # the direction out of the nose\n",
    "    p1 = ( int(image_points[33][0]), int(image_points[33][1]) )\n",
    "    p2 = ( int(nose_end_point2D[0][0][0]), int(nose_end_point2D[0][0][1]) )\n",
    "    cv2.line(im, p1, p2, (255,0,0), 2)\n",
    "    \n",
    "    # Display image\n",
    "    cv2.imshow(output, im)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = glob.glob('dataset/*')\n",
    "print(len(participants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training examples to use(0-2758)\n",
    "DATASET_SIZE = 2728\n",
    "DEBUG = False\n",
    "\n",
    "# Load the dataset\n",
    "with open('data_cleaned.json') as json_file:\n",
    "    data_all = json.load(json_file)\n",
    "# Extract the keys in sorted order\n",
    "keys_all = sorted(data_all)\n",
    "# Convert python list to np array\n",
    "keys_all = np.asarray(keys_all)\n",
    "print(len(keys_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy metrics for the whole dataset. These are compouter\n",
    "# by leaving every Subject out one time, calculating the accuracy for each\n",
    "# one and then taking the mean.\n",
    "accuracy_rf_total = 0\n",
    "accuracy_svm_total = 0\n",
    "precision_rf_total = 0\n",
    "precision_svm_total = 0\n",
    "recall_rf_total = 0\n",
    "recall_svm_total = 0\n",
    "\n",
    "model_points = get_full_model_points()\n",
    "\n",
    "# Array to keep track of subjects with low score\n",
    "low_score_subjects_rf = []\n",
    "low_score_subjects_svm = []\n",
    "\n",
    "for j in range(len(participants)):\n",
    "    uuid_excluded = participants[j].split('/')[1]\n",
    "\n",
    "    indices_excluded = []\n",
    "    keys_excluded = []\n",
    "    for i in range(DATASET_SIZE):\n",
    "        key = keys_all[i]\n",
    "        uuid = key.split('/')[0]\n",
    "        if(uuid == uuid_excluded):\n",
    "            indices_excluded.append(i)\n",
    "            keys_excluded.append(key)\n",
    "    keys = np.delete(keys_all, indices_excluded)\n",
    "\n",
    "    CURRENT_DATASET_SIZE = keys.shape[0]\n",
    "\n",
    "    X = np.zeros((CURRENT_DATASET_SIZE, 14, 1))\n",
    "    y = np.zeros(CURRENT_DATASET_SIZE)\n",
    "\n",
    "    # Indices that the SolvePnP failed\n",
    "    failed_indices = []\n",
    "\n",
    "    for i in range(CURRENT_DATASET_SIZE):\n",
    "        key = keys[i]\n",
    "\n",
    "        # Approximate camera intrinsic parameters\n",
    "#         im = cv2.imread('dataset/' + key)   # This imread is time consuming! Another way?\n",
    "#         size = im.shape\n",
    "        with open('dataset/' + key.split('/')[0] + '/data.yml', 'r') as stream:\n",
    "            try:\n",
    "                laptop = yaml.safe_load(stream)['laptop']\n",
    "                if(laptop == 'k' or laptop == 'K'):\n",
    "                    size = (480, 640, 3)\n",
    "                elif(laptop == 'c' or laptop == 'C'):\n",
    "                    size = (720, 1280, 3)\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(exc)\n",
    "\n",
    "        landmarks = data_all[key]['landmarks']\n",
    "        \n",
    "        camera_matrix, dist_coeffs = get_camera_parameters(size)\n",
    "        \n",
    "        image_points = get_full_image_points(landmarks)\n",
    "\n",
    "        # Solve the PnP problem with the parameters specified above\n",
    "        # and obtain rotation and translation vectors\n",
    "        (success, rotation_vector, translation_vector) = cv2.solvePnP(\n",
    "            model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE\n",
    "            )\n",
    "        \n",
    "        # Data from Architecture #2. Reshape is done for compatibility reasons\n",
    "        iris_right = np.reshape(np.asarray(data_all[key]['iris_right']), (2, 1))\n",
    "        iris_left = np.reshape(np.asarray(data_all[key]['iris_left']), (2, 1))\n",
    "\n",
    "        # Data from Architecture #3\n",
    "        left_vector = np.asarray( (abs(iris_left[0] - landmarks[39]['x']), abs(iris_left[1] - landmarks[39]['y'])) )\n",
    "        right_vector = np.asarray( (abs(iris_right[0] - landmarks[42]['x']), abs(iris_right[1] - landmarks[42]['y'])) )\n",
    "\n",
    "        X[i, :] = np.concatenate((rotation_vector, translation_vector, iris_left, iris_right, \n",
    "                                 left_vector, right_vector), axis=0)\n",
    "\n",
    "        # Check if it is positive or negative example\n",
    "        output = key.split('/')[1]\n",
    "        if(output == 'positive'):\n",
    "            y[i] = 1\n",
    "        elif(output == 'negative'):\n",
    "            y[i] = 0\n",
    "    \n",
    "        # Remove examples that SolvePnP crashed\n",
    "        if(X[i, 0] > 10000):\n",
    "            print(key)\n",
    "            failed_indices.append(i)\n",
    "    X = np.delete(X, failed_indices, axis=0)\n",
    "    y = np.delete(y, failed_indices, axis=0)\n",
    "    \n",
    "    X = X.squeeze()\n",
    "\n",
    "    m = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    \n",
    "    X_scaled = (X - m)/std\n",
    "    \n",
    "    ### Train and Predict\n",
    "    X_train, y_train = X_scaled, y\n",
    "\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=500, random_state=1)\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    svm_classifier = svm.SVC(C=10, kernel='rbf', gamma='scale', probability=True)\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "#     with open('classifiers/rf/' + uuid_excluded + '.pickle', 'wb') as f:\n",
    "#         pickle.dump(rf_classifier, f, pickle.HIGHEST_PROTOCOL)\n",
    "#     with open('classifiers/svm/' + uuid_excluded + '.pickle', 'wb') as f:\n",
    "#         pickle.dump(svm_classifier, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # Retrieve the saved classifiers\n",
    "#     with open('classifiers/rf/' + uuid_excluded + '.pickle', 'rb') as f:\n",
    "#         rf_classifier = pickle.load(f)\n",
    "#     with open('classifiers/svm/' + uuid_excluded + '.pickle', 'rb') as f:\n",
    "#         svm_classifier = pickle.load(f)\n",
    "\n",
    "# Construct the validation dataset\n",
    "    X_eval = np.zeros((len(keys_excluded), 14, 1))\n",
    "    y_eval = np.zeros(len(keys_excluded))\n",
    "\n",
    "    for i in range(len(keys_excluded)):\n",
    "        key = keys_excluded[i]\n",
    "\n",
    "#         im = cv2.imread('dataset/' + key)   # This imread is time consuming! Another way?\n",
    "#         size = im.shape\n",
    "        with open('dataset/' + key.split('/')[0] + '/data.yml', 'r') as stream:\n",
    "            try:\n",
    "                laptop = yaml.safe_load(stream)['laptop']\n",
    "                if(laptop == 'k' or laptop == 'K'):\n",
    "                    size = (480, 640, 3)\n",
    "                elif(laptop == 'c' or laptop == 'C'):\n",
    "                    size = (720, 1280, 3)\n",
    "                else:\n",
    "                    print('Failed to load size')\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(exc)\n",
    "\n",
    "        landmarks = data_all[key]['landmarks']\n",
    "        \n",
    "        camera_matrix, dist_coeffs = get_camera_parameters(size)\n",
    "        \n",
    "        image_points = get_full_image_points(landmarks)\n",
    "\n",
    "        # Solve the PnP problem with the parameters specified above\n",
    "        # and obtain rotation and translation vectors\n",
    "        (success, rotation_vector, translation_vector) = cv2.solvePnP(\n",
    "            model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE\n",
    "            )\n",
    "\n",
    "        # Data from Architecture #2. Reshape is done for compatibility reasons\n",
    "        iris_right = np.reshape(np.asarray(data_all[key]['iris_right']), (2, 1))\n",
    "        iris_left = np.reshape(np.asarray(data_all[key]['iris_left']), (2, 1))\n",
    "\n",
    "        # Data from Architecture #3\n",
    "        left_vector = np.asarray( (abs(iris_left[0] - landmarks[39]['x']), abs(iris_left[1] - landmarks[39]['y'])) )\n",
    "        right_vector = np.asarray( (abs(iris_right[0] - landmarks[42]['x']), abs(iris_right[1] - landmarks[42]['y'])) )\n",
    "\n",
    "        X_eval[i, :] = np.concatenate((rotation_vector, translation_vector, iris_left, iris_right, \n",
    "                                      left_vector, right_vector), axis=0)\n",
    "        \n",
    "        # Check if it is positive or negative example\n",
    "        output = key.split('/')[1]\n",
    "        if(output == 'positive'):\n",
    "            y_eval[i] = 1\n",
    "        elif(output == 'negative'):\n",
    "            y_eval[i] = 0\n",
    "\n",
    "    X_eval = X_eval.squeeze()\n",
    "\n",
    "    # Normalization\n",
    "    m_eval = X_eval.mean(axis=0)\n",
    "    std_eval = X_eval.std(axis=0)\n",
    "    X_eval = (X_eval - m_eval)/std_eval\n",
    "\n",
    "    # Predict Random Forest\n",
    "    y_pred_rf = rf_classifier.predict(X_eval)\n",
    "    rf_accuracy_subject = metrics.accuracy_score(y_eval, y_pred_rf)\n",
    "\n",
    "    # Predict SVM\n",
    "    threshold = 0.3\n",
    "    y_prob_svm = svm_classifier.predict_proba(X_eval)\n",
    "    y_pred_svm = (y_prob_svm[:, 1] >= threshold).astype(int)\n",
    "    svm_accuracy_subject = metrics.accuracy_score(y_eval, y_pred_svm)\n",
    "    \n",
    "    confusion_matrix_rf = metrics.confusion_matrix(y_eval, y_pred_rf)\n",
    "    confusion_matrix_svm = metrics.confusion_matrix(y_eval, y_pred_svm)\n",
    "    \n",
    "    precision_rf = confusion_matrix_rf[1][1]/(confusion_matrix_rf[1][1] + confusion_matrix_rf[0][1])\n",
    "    recall_rf = confusion_matrix_rf[1][1]/(confusion_matrix_rf[1][1] + confusion_matrix_rf[1][0])\n",
    "    precision_svm = confusion_matrix_svm[1][1]/(confusion_matrix_svm[1][1] + confusion_matrix_svm[0][1])\n",
    "    recall_svm = confusion_matrix_svm[1][1]/(confusion_matrix_svm[1][1] + confusion_matrix_svm[1][0])\n",
    "    \n",
    "    print('RF  #{} Accuracy: {} | Precision: {} | Recall: {}'.format(j, round(rf_accuracy_subject,3),\n",
    "                                                            round(precision_rf,2), round(recall_rf, 2)))\n",
    "    print('SVM #{} Accuracy: {} | Precision: {} | Recall: {}'.format(j, round(svm_accuracy_subject, 3),\n",
    "                                                            round(precision_svm, 2), round(recall_svm, 2)))\n",
    "\n",
    "    if(rf_accuracy_subject <= 0.5):\n",
    "        low_score_subjects_rf.append(uuid_excluded)\n",
    "        with open('classifiers/rf/' + uuid_excluded + '.pickle', 'wb') as f:\n",
    "            pickle.dump(rf_classifier, f, pickle.HIGHEST_PROTOCOL)\n",
    "    if(svm_accuracy_subject <= 0.5):\n",
    "        low_score_subjects_svm.append(uuid_excluded)\n",
    "        with open('classifiers/svm/' + uuid_excluded + '.pickle', 'wb') as f:\n",
    "            pickle.dump(svm_classifier, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    accuracy_rf_total += rf_accuracy_subject*len(keys_excluded)\n",
    "    accuracy_svm_total += svm_accuracy_subject*len(keys_excluded)\n",
    "    precision_rf_total += precision_rf*len(keys_excluded)\n",
    "    precision_svm_total += precision_svm*len(keys_excluded)\n",
    "    recall_rf_total += recall_rf*len(keys_excluded)\n",
    "    recall_svm_total += recall_svm*len(keys_excluded)\n",
    "    \n",
    "accuracy_rf_total /= DATASET_SIZE\n",
    "accuracy_svm_total /= DATASET_SIZE\n",
    "precision_rf_total /= DATASET_SIZE\n",
    "precision_svm_total /= DATASET_SIZE\n",
    "recall_rf_total /= DATASET_SIZE\n",
    "recall_svm_total /= DATASET_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Accuracy RF:  {}'.format(accuracy_rf_total))\n",
    "print('Total Accucary SVM: {}'.format(accuracy_svm_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision RF: {}'.format(precision_rf_total))\n",
    "print('Precision SVM: {}'.format(precision_svm_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Recall RF: {}'.format(recall_rf_total))\n",
    "print('Recall SVM: {}'.format(recall_svm_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(low_score_subjects_rf))\n",
    "print(len(low_score_subjects_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"low_score_rf.txt\", \"w\") \n",
    "for i in low_score_subjects_rf:\n",
    "    file1.write(i + '\\n')\n",
    "    \n",
    "file2 = open(\"low_score_svm.txt\", \"w\")\n",
    "for i in low_score_subjects_svm:\n",
    "    file2.write(i + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_scaled[:, 0:14].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
